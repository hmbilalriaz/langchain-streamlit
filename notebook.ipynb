{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex Version: 0.12.19\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "from importlib.metadata import version\n",
    "print(\"LlamaIndex Version:\", version(\"llama_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/Projects/python-ml/langchain-ollama-streamlit/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = Ollama(model='deepseek-r1:latest', request_timeout=120.0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name='sentence-transformers/all-MiniLM-L6-v2', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the directory path\n",
    "input_dir_path = \"chunks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "#Load Data \n",
    "loader = SimpleDirectoryReader(input_dir=input_dir_path, required_exts=['.pdf'], recursive=True)\n",
    "\n",
    "docs = loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='c9755c72-75d2-4cf4-ade0-379285232e57', embedding=None, metadata={'page_label': '1', 'file_name': 'DSPy.pdf', 'file_path': '/Users/macbookpro/Projects/python-ml/langchain-ollama-streamlit/chunks/DSPy.pdf', 'file_type': 'application/pdf', 'file_size': 401852, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"DSPy - Programming—not prompting—LMs \\nDSPy is the framework for programming—rather than prompting—language models. It allows \\nyou to iterate fast on building modular AI systems and offers algorithms for optimizing their \\nprompts and weights, whether you're building simple classifiers, sophisticated RAG pipelines, or \\nAgent loops. \\nDSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write \\ncompositional Python code and use DSPy to teach your LM to deliver high-quality outputs. \\n \\nGetting Started I: Install DSPy and set up your LM \\n> pip install -U dspy \\n \\nLocal LMs on your laptop \\nFirst, install Ollama and launch its server with your LM. \\n> curl -fsSL https://ollama.ai/install.sh | sh \\n> ollama run llama3.2:1b  \\nThen, connect to it from your DSPy code. \\nimport dspy \\nlm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='') \\ndspy.configure(lm=lm) \\n \\n \\n \", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy is available: 1.26.4\n",
      "PyTorch is available: 2.2.2\n",
      "Index created successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Numpy is available:\", np.__version__)\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch is available:\", torch.__version__)\n",
    "except ImportError: \n",
    "    print(\"PyTorch is not available\")               \n",
    "\n",
    "try:\n",
    "    #Create vector store index\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    print(\"Index created successfully\")\n",
    "except RuntimeError as e:\n",
    "    print(\"Error creating index:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x138650df0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what DSPy is based on the context provided. Let me read through the information again.\n",
      "\n",
      "The context starts by introducing a file calledDSPy.pdf and setting up some prerequisites like installing Ollama and using pip to install dspy. Then it explains what DSPy does. \n",
      "\n",
      "From the first paragraph, I gather that DSPy is a framework for programming language models in a different way than just prompting them. Instead of telling the model what to do with prompts, you write Python code. So it's about building modular AI systems and optimizing their prompts and weights.\n",
      "\n",
      "DSPy stands for Declarative Self-improving Python, which I think means that instead of using rigid prompts, you write Python code that's more declarative, letting the model figure things out on its own through iteration and optimization. This allows for faster development and better performance in tasks like classification, RAG pipelines, or agent loops.\n",
      "\n",
      "The \"Getting Started\" part mentions installing dspy with pip install -U dspy and using it to connect to an Ollama local LM from Python code after setting up the necessary environment with curl commands.\n",
      "\n",
      "Putting this together, I understand that DSPy is a tool that helps in creating more efficient and adaptable AI systems by letting you write Python code instead of traditional prompts. It's designed for building modular systems where the model can optimize itself through its code.\n",
      "</think>\n",
      "\n",
      "DSPy is a framework designed to enable programming language models declaratively rather than using traditional prompting. By writing compositional Python code, users can create modular AI systems that are optimized through algorithms provided by DSPy, supporting tasks like classification and RAG pipelines.\n",
      "\n",
      "**Answer:**  \n",
      "DSPy is a framework for building modular AI systems using Python code, allowing you to program language models in an declarative way without traditional prompts. It supports efficient development of AI systems such as classifiers and RAG pipelines by optimizing their prompts and weights.\n"
     ]
    }
   ],
   "source": [
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What exactly is DSPy?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to install DSPy. From what I remember, the context provided mentions that DSPy is a framework for programming language models instead of prompting them. But right now, my focus is on installation.\n",
      "\n",
      "Looking at the context, under \"Getting Started I,\" it says that you can install DSPy using pip with the command \"pip install -U dspy.\" That seems straightforward enough. So step one would be to open my terminal or command prompt and run that pip install command.\n",
      "\n",
      "Wait, but sometimes package names can change or there might be different ways depending on whether I'm on a Mac, Windows, or Linux. However, the context doesn't specify platform differences for installing DSPy, so I'll assume it's compatible across common platforms with the same installation method.\n",
      "\n",
      "So to break it down: first, make sure my Python environment is set up correctly if using virtual environments because pip installs in the current environment by default. But the context doesn't mention any issues with that, so maybe it's fine as is.\n",
      "\n",
      "After installing dspy via pip, I might need to configure the language model within DSPy. The example shows importing dspy and creating an LM instance with specific parameters like 'ollama_chat/llama3.2' and setting up the API details. That part isn't directly related to installation but more about how to connect after it's installed.\n",
      "\n",
      "So in summary, the primary step is installing dspy using pip. I don't think there are any additional dependencies mentioned beyond what pip provides for this installation. Therefore, the main action needed is running that one command.\n",
      "</think>\n",
      "\n",
      "To install DSPy, execute the following command in your terminal or command prompt:\n",
      "\n",
      "```bash\n",
      "pip install -U dspy\n",
      "```\n",
      "\n",
      "This will install the latest version of DSPy using pip. After installation, you can configure the language model within DSPy as described in the provided context.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('How to install DSPy')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
