{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain Version: 0.12.19\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "from importlib.metadata import version\n",
    "print(\"LlamaIndex Version:\", version(\"llama_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from importlib.metadata import version\n",
    "print(\"Langchain Version:\", version(\"llama_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "pdf_loader = PyPDFLoader(\"chunks/DSPy.pdf\")\n",
    "pdf_docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'chunks/' directory exists\n",
    "os.makedirs(\"chunks\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted and saved 2 chunks in 'chunks/' directory.\n"
     ]
    }
   ],
   "source": [
    "# Save chunks to files\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_path = f\"chunks/chunk_{i}.txt\"\n",
    "    with open(chunk_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(chunk.page_content)\n",
    "print(f\"âœ… Extracted and saved {len(chunks)} chunks in 'chunks/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant = QdrantClient(\":memory:\")  # Use persistent DB if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collection name\n",
    "collection_name = \"naive_rag_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/4gplhqk164g40d8zvhlkrvyc0000gn/T/ipykernel_64437/588391254.py:2: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Qdrant Collection (Ensuring it Exists)\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedding Model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Vector Store\n",
    "vector_store = Qdrant(\n",
    "    client=qdrant,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text chunks into LangChain Document format\n",
    "docs_to_store = [Document(page_content=chunk.page_content, metadata={\"source\": f\"chunk_{i}.txt\"}) for i, chunk in enumerate(chunks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully stored 2 chunks in Qdrant.\n"
     ]
    }
   ],
   "source": [
    "# Store Documents in Qdrant\n",
    "vector_store.add_documents(docs_to_store)\n",
    "\n",
    "print(f\"âœ… Successfully stored {len(docs_to_store)} chunks in Qdrant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom prompt (optional, but recommended)\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/4gplhqk164g40d8zvhlkrvyc0000gn/T/ipykernel_64437/1977204237.py:2: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  combine_documents_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=custom_prompt)\n"
     ]
    }
   ],
   "source": [
    "# Load a standard QA chain\n",
    "combine_documents_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=custom_prompt)\n",
    "\n",
    "# Create Naive RAG pipeline correctly\n",
    "qa_chain = RetrievalQAWithSourcesChain(\n",
    "    retriever=retriever,  # Correct way to pass retriever\n",
    "    combine_documents_chain=combine_documents_chain,  # Required in latest versions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Answer: {'question': 'What is DSPy?', 'answer': '<think>\\nOkay, so I need to figure out what DSPy is based on the context provided. Let me read through it carefully.\\n\\nThe first sentence says, \"DSPy - Programmingâ€”not promptingâ€”LMs.\" So right away, I know that DSPy isn\\'t about prompting language models directly but instead something else related to programming them or working with them in a different way.\\n\\nNext, it explains that DSPy is the framework for programming language models. Instead of using prompts like you would in a traditional AI setup, you use declarative Python code. That makes me think it\\'s more about writing code than giving commands through text prompts.\\n\\nThen it mentions thatDSPy allows iterating quickly on building modular AI systems. Modular systems are those that can be built by connecting different parts or components together without too much interference from each other. So this framework probably helps in creating such systems efficiently.\\n\\nIt also talks about algorithms for optimizing prompts and weights, which makes me think of machine learning optimizations. Whether you\\'re dealing with simple classifiers, RAG (Retrieval-Augmented Generation) pipelines, or more complex agent loops, DSPy has tools to help optimize these aspects. So it\\'s not just a one-size-fits-all solution but something that can adapt to different AI systems.\\n\\nDSPy stands for Declarative Self-improving Python. That seems like a key part of its functionality. Instead of using brittle prompts (which are probably unreliable or unstable prompt structures), you write compositional Python code. Compositional suggests that it\\'s built from smaller parts, which can be combined in various ways without issues.\\n\\nThe \"Declarative Self-improving\" part is interesting. Declarative implies that you\\'re making statements rather than giving step-by-step instructions, while self-improving suggests that the system can enhance itself over time. So perhaps it\\'s an AI framework that gets better as it uses it more, without needing constant manual updates.\\n\\nThe helpful answer provided says that DSPy is a framework for programming language models by writing compositional Python code and optimizing prompts and weights. It doesn\\'t use direct prompting but declarative code to teach the model high-quality outputs.\\n\\nPutting this all together, I understand that DSPy is an alternative approach to interacting with AI models like LLMs (Large Language Models). Instead of using typical text-based prompts, it uses Python code that\\'s more structured and self-improving. This allows for faster development of modular systems and optimizes how the model processes inputs and outputs.\\n\\nSo, in summary, DSPy is a tool that lets you program AI models by writing Python code rather than giving direct commands, enabling quicker building of complex systems and optimizing their performance.\\n</think>\\n\\nDSPy is a framework designed to interact with large language models (LLMs) through declarative Python programming. Unlike traditional prompting, it uses structured Python code for better control and efficiency. DSPy facilitates the creation of modular AI systems by allowing rapid iteration and optimization of model interactions. It supports various AI components like classifiers and agent loops, providing tools for enhancing prompts and weights. By leveraging self-improving capabilities, DSPy aids in developing high-performance AI applications without manual intervention.', 'sources': ''}\n"
     ]
    }
   ],
   "source": [
    "# Run a query\n",
    "query = \"What is DSPy?\"\n",
    "response = qa_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"ðŸ“Œ Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Here on, I've implemented using LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "from importlib.metadata import version\n",
    "print(\"LlamaIndex Version:\", version(\"llama_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/Projects/python-ml/langchain-ollama-streamlit/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = Ollama(model='deepseek-r1:latest', request_timeout=120.0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name='sentence-transformers/all-MiniLM-L6-v2', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the directory path\n",
    "input_dir_path = \"chunks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "#Load Data \n",
    "loader = SimpleDirectoryReader(input_dir=input_dir_path, required_exts=['.pdf'], recursive=True)\n",
    "\n",
    "docs = loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='c9755c72-75d2-4cf4-ade0-379285232e57', embedding=None, metadata={'page_label': '1', 'file_name': 'DSPy.pdf', 'file_path': '/Users/macbookpro/Projects/python-ml/langchain-ollama-streamlit/chunks/DSPy.pdf', 'file_type': 'application/pdf', 'file_size': 401852, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"DSPy - Programmingâ€”not promptingâ€”LMs \\nDSPy is the framework for programmingâ€”rather than promptingâ€”language models. It allows \\nyou to iterate fast on building modular AI systems and offers algorithms for optimizing their \\nprompts and weights, whether you're building simple classifiers, sophisticated RAG pipelines, or \\nAgent loops. \\nDSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write \\ncompositional Python code and use DSPy to teach your LM to deliver high-quality outputs. \\n \\nGetting Started I: Install DSPy and set up your LM \\n> pip install -U dspy \\n \\nLocal LMs on your laptop \\nFirst, install Ollama and launch its server with your LM. \\n> curl -fsSL https://ollama.ai/install.sh | sh \\n> ollama run llama3.2:1b  \\nThen, connect to it from your DSPy code. \\nimport dspy \\nlm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='') \\ndspy.configure(lm=lm) \\n \\n \\n \", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy is available: 1.26.4\n",
      "PyTorch is available: 2.2.2\n",
      "Index created successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Numpy is available:\", np.__version__)\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch is available:\", torch.__version__)\n",
    "except ImportError: \n",
    "    print(\"PyTorch is not available\")               \n",
    "\n",
    "try:\n",
    "    #Create vector store index\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    print(\"Index created successfully\")\n",
    "except RuntimeError as e:\n",
    "    print(\"Error creating index:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x138650df0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what DSPy is based on the context provided. Let me read through the information again.\n",
      "\n",
      "The context starts by introducing a file calledDSPy.pdf and setting up some prerequisites like installing Ollama and using pip to install dspy. Then it explains what DSPy does. \n",
      "\n",
      "From the first paragraph, I gather that DSPy is a framework for programming language models in a different way than just prompting them. Instead of telling the model what to do with prompts, you write Python code. So it's about building modular AI systems and optimizing their prompts and weights.\n",
      "\n",
      "DSPy stands for Declarative Self-improving Python, which I think means that instead of using rigid prompts, you write Python code that's more declarative, letting the model figure things out on its own through iteration and optimization. This allows for faster development and better performance in tasks like classification, RAG pipelines, or agent loops.\n",
      "\n",
      "The \"Getting Started\" part mentions installing dspy with pip install -U dspy and using it to connect to an Ollama local LM from Python code after setting up the necessary environment with curl commands.\n",
      "\n",
      "Putting this together, I understand that DSPy is a tool that helps in creating more efficient and adaptable AI systems by letting you write Python code instead of traditional prompts. It's designed for building modular systems where the model can optimize itself through its code.\n",
      "</think>\n",
      "\n",
      "DSPy is a framework designed to enable programming language models declaratively rather than using traditional prompting. By writing compositional Python code, users can create modular AI systems that are optimized through algorithms provided by DSPy, supporting tasks like classification and RAG pipelines.\n",
      "\n",
      "**Answer:**  \n",
      "DSPy is a framework for building modular AI systems using Python code, allowing you to program language models in an declarative way without traditional prompts. It supports efficient development of AI systems such as classifiers and RAG pipelines by optimizing their prompts and weights.\n"
     ]
    }
   ],
   "source": [
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What exactly is DSPy?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to install DSPy. From what I remember, the context provided mentions that DSPy is a framework for programming language models instead of prompting them. But right now, my focus is on installation.\n",
      "\n",
      "Looking at the context, under \"Getting Started I,\" it says that you can install DSPy using pip with the command \"pip install -U dspy.\" That seems straightforward enough. So step one would be to open my terminal or command prompt and run that pip install command.\n",
      "\n",
      "Wait, but sometimes package names can change or there might be different ways depending on whether I'm on a Mac, Windows, or Linux. However, the context doesn't specify platform differences for installing DSPy, so I'll assume it's compatible across common platforms with the same installation method.\n",
      "\n",
      "So to break it down: first, make sure my Python environment is set up correctly if using virtual environments because pip installs in the current environment by default. But the context doesn't mention any issues with that, so maybe it's fine as is.\n",
      "\n",
      "After installing dspy via pip, I might need to configure the language model within DSPy. The example shows importing dspy and creating an LM instance with specific parameters like 'ollama_chat/llama3.2' and setting up the API details. That part isn't directly related to installation but more about how to connect after it's installed.\n",
      "\n",
      "So in summary, the primary step is installing dspy using pip. I don't think there are any additional dependencies mentioned beyond what pip provides for this installation. Therefore, the main action needed is running that one command.\n",
      "</think>\n",
      "\n",
      "To install DSPy, execute the following command in your terminal or command prompt:\n",
      "\n",
      "```bash\n",
      "pip install -U dspy\n",
      "```\n",
      "\n",
      "This will install the latest version of DSPy using pip. After installation, you can configure the language model within DSPy as described in the provided context.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('How to install DSPy')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
